# Horse Racing Outcome Prediction â€” Deep Learning vs Random Forest

This repository contains a deep learning group project analyzing and predicting **Hong Kong horse racing outcomes** using the Kaggle â€œHK Horse Racingâ€ dataset. This project was completed as part of "STAT4012 â€” Deep Learning" at The Chinese University of Hong Kong (CUHK).
We develop and compare two approaches:

- **Random Forest classifier** (baseline model)  
- **Deep Neural Network (Keras / TensorFlow)** (main model)

The goal is to examine whether flexible nonlinear models can outperform classical tree-based methods in predicting the winner and evaluating finishing positions (Top-3 / Top-4).

## ğŸ“… Project Information
- Completion date: May 2021
- Languages: Python

## ğŸš€ Project Overview

**Objective:**  
Predict the finishing performance of each horse in a race using historical race-level and horse-level features.

**Dataset:**  
- `races.csv` â€” race characteristics (venue, distance, going, class, course configuration, etc.)  
- `runs.csv` â€” horse-level information (age, weight, draw, country, odds, past performance)

Data source: Kaggle (Hong Kong Horse Racing dataset).  
Raw data are not included in this repo due to size and licensing; see `data/README.md` for download instructions.

---

## ğŸ§  Methods Summary

### 1. **Feature Engineering**
- Merge race and horse records  
- Encode categorical variables  
- Rank-encode country/type based on performance  
- Standardize and flatten features to form a fixed-length vector (up to 14 horses per race)  
- Construct labels:
  - Winner (one-hot)
  - Top-3 / Top-4 indicators

### 2. **Random Forest (Baseline)**
Implemented in `src/horse_random_forest.py`:
- Grid-search tuning  
- PCA variant tested  
- Evaluated on winner prediction and Top-3 accuracy

### 3. **Deep Neural Network (Main Model)**
Implemented in `src/horse_dnn.py`:
- 1 hidden layer, 512 neurons, `tanh` activation  
- Softmax output (14 classes)  
- Adam optimizer (`lr = 0.0005`)  
- Trained for 30 epochs with 5-fold cross-validation

---

## ğŸ“ˆ Key Results

| Model                    | Winner Accuracy | Top-3 Accuracy |
|--------------------------|-----------------|----------------|
| Random Forest            | ~0.08â€“0.09      | ~0.22â€“0.24     |
| **Deep Neural Network**  | **~0.18**       | **~0.46â€“0.47** |

ğŸ” **Insights:**
- Winner prediction is hard due to near-random inherent structure (1/14 baseline).  
- DNN captures nonlinear dependencies better than RF, showing substantial gains in Top-3 performance.  
- Feature flattening + deep models work reasonably well despite the datasetâ€™s noisy nature.

Full analysis is in `report/final_report.pdf`.

---

## ğŸ“ Repository Structure
.
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ horse_dnn.py # Deep learning model (Keras)
â”‚ â””â”€â”€ horse_random_forest.py # Random Forest baseline
â”‚
â”œâ”€â”€ report/
â”‚ â””â”€â”€ final_report.pdf # Full project write-up
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ README.md # Instructions to obtain dataset
â”‚
â””â”€â”€ README.md # Project documentation

---

## â–¶ï¸ How to Run

### 1. Install dependencies  
(Example using conda)

```
conda create -n horse python=3.11
conda activate horse
pip install -r requirements.txt
```
2. Download Kaggle data

See data/README.md.

3. Run models
```
python src/horse_random_forest.py
python src/horse_dnn.py
```
